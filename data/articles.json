{"_id":{"$oid":"633beee25337fd7e7233174e"},"title":"cloud computing is everywhere ","description":"Cloud will be synonymous with the word Web. Can you think of a business today that‚Äôs not on the internet? ","creator":{"$oid":"633beddc5337fd7e72331748"},"content":"                  To understand the cloud, let‚Äôs talk about the computer in your home. It is primarily composed of 3 things. A storage system for storing data. Some chips on the motherboards to perform tasks (tasks like moving data around or running some algorithms), and a user interface device to show the data to the user (computer monitor) through which the user can interact with the system (keyboard and mouse)\n\nThe first thing that people started needing more is storage capacity. Back then, like 7 years back, you probably had more external hard drives than you had computers in your home. Cameras started to increase in megapixel count, and all those home videos of birthdays and family trips on digital video cameras needed a lot of space to store.\n\nManaging hard drives and worrying about backups in case they failed were a constant headache. If you ever bought Seagate hard drives, you already know.\n\nThen Dropbox came along where you could buy some storage space online and upload your files to Dropbox. They maintained backups so you didn‚Äôt need to worry about losing your data or keeping manual backups. They appeared as another drive on your computer just like if you had plugged your external hard drive in.\n\nNo more external drives to manage or USB cables to wrangle or running external power supplies. Your data on someone‚Äôs else servers under your name.\n\nThe one thing you worried about was security and that has come a long way. We‚Äôll talk about that later in this article.\n\nToday we pay $1/month for 1TB of data to Google to buy storage on Google drive. A 1TB Western Digital hard drive would cost about $55. That‚Äôs buying 4 years worth of storage on the cloud for which you don‚Äôt need to worry about backups, hard drive crashes, or worst ‚Äî the problem of whatever version of windows would be there 4 years from now not recognizing your USB drive anymore!\n\nWhile consumers started offloading their storage to the cloud, companies started thinking about how to offload more than just their data.\nBig companies have massive data centers ‚Äî buildings full of servers. Medium-sized companies have 1 room full of servers called the server room. In addition to data, companies have another problem ‚Äî they need to estimate how much compute power they‚Äôd need on their servers. This is the most challenging part. Adding more hard drives to satisfy storage needs is the easy part.\n\nServers in a company serve the internal world ‚Äî the employees, and the external world ‚Äî the customers or users of the company‚Äôs product.\n\nWhen the company is being set up or before a product is launched, someone (usually the CTO or the VP of Engineering or the CIO) would need to estimate the number of servers they‚Äôd need to be able to handle the estimated number of customers.\n\nIt is a big deal to order servers. They are huge, they are expensive, they take months to get delivered, they need to be installed by experts, their physical backups need to be installed at the same time at some other location, and the proper cooling for servers need to be installed in the room (Servers run 24/7 and they need to stay cool‚Ä¶very cool. This is why you can see your breath in a server room.) along with thousands of wires snaking through the walls that are feeding-tubes for all the computers that these servers are serving inside the company.\n\nAdd to that, the cost of buying expensive server software licenses, paying exorbitant fees for database management systems, electrical panels to support the high wattage, extra security system for server rooms and hiring the right people to set up and maintain these servers, networking, firewall, and work the software.","ttr":"10 mins read","image":"https://thomastheblogger.s3.ap-south-1.amazonaws.com/61903727.jpeg","createdAt":{"$date":"2022-10-04T08:21:54.614Z"},"__v":0}
{"_id":{"$oid":"633bf6ad5337fd7e72331788"},"title":"Smuggling the HTTP Protocol","description":"How to attack the http protocol to smuggle your payload through WAF","creator":{"$oid":"633bf5b85337fd7e7233177f"},"content":"If you already know what is HTTP Request Smuggling you can skip this section but if you want to know the basics I‚Äôd recommend read carefully.\r\n\r\nIn this section I‚Äôll try to put everyone under the same page covering only the basics about HTTP Request Smuggling.\r\n\r\nIn August 2019 when James Kettle brought HTTP Request Smuggling back from the ashes I tried to understand this vulnerability and at that time it was difficult to me understand everything.\r\n\r\nNow after exploiting a few instances I see the problem to understand at the first glance. Most of the time we are looking for a vulnerability on the application and HTTP Request Smuggling also involves another layer called network.\r\n\r\nThe images from now one in this section are from this YouTube video ‚Äú$6,5k + $5k HTTP Request Smuggling mass account takeover ‚Äî Slack + Zomato‚Äù. Thanks Grzegorz Niedziela for allowing me to use the images! I strong recommend you to watch this video after reading this post.\r\n\r\nBefore talking about HTTP Request Smuggling itself lets recap some features from HTTP protocol version 1.1. A HTTP server can process multiple requests under the same TCP connection as you can see in the example below.\r\nThe header Content-Length defines the size of the body which tells to the server where the body finishes. There is another header called Transfer-Encoding which also defines the size of the body.\r\nThe Transfer-Encoding header indicates the body will be sent in chunks and the numbers in the beginning of each chunk indicates the size of it in a hexadecimal format. The last chunk should be indicate with number 0 which determines the end of the body.\r\n\r\nThe main difference between Content-Length and Transfer-Encoding is in the first case the request send the entire body at once and on Transfer-Encoding the body is sent in pieces.\r\n\r\nBut what happen when both headers are present?\r\nRFC describes the beauty of the theory but this is not what happen in practice. When an environment do not respect the sentence above the HTTP Request Smuggling is possible.\r\n\r\nNowadays is pretty common to see web applications in the back-end and a reverse proxy in the front-end like the diagram below.\r\nIn the image above we can see Bob and Alice requests one next to another. The Bob‚Äôs request comes first and the front-end is using the Content-Length header (ignoring Transfer-Encoding) to defines the body length which means for the front-end Bob‚Äôs request ends right after the text key=value and Alice‚Äôs request starts at POST / HTTP/1.1.\r\n\r\nIn the other side back-end is using Transfer-Encoding header (ignoring Content-Length) and defining the end of Bob‚Äôs request at the number 0 and assuming the Alice‚Äôs requests starts with the text key=value which is an invalid request.\r\n\r\nIf Bob is a skilled attacker he can craft a malicious request and force Alice to receives a different response from what was supposed to be the original response from Alice‚Äôs request.\r\n\r\nThat‚Äôs the most important part of HTTP Request Smuggling. If you didn‚Äôt get what is happening here I strong recommend you go back and read everything again.\r\nReporting HTTP Request Smuggling üìù\r\n\r\nI was scanning some subdomains using Smuggler in a private bug bounty program on Hackerone when I initially found 13 subdomains reported as potential vulnerable to HTTP Request Smuggling by Smuggler. I reported all of them in one single report as critical even without a real PoC because I was afraid to get a duplicate and decided to work on the impact later. I got that felling there was something big which would require time to investigate.\r\n\r\nIf you already ran Smuggler before you probably know most of the time Smuggler reports as potential vulnerable but you cannot really get any real impact directly. For each case a research is required to understand the context and test a malicious scenario to prove the impact.\r\n\r\nThe most common impact that I‚Äôve seen it is what I called as Universal Redirect. Universal Redirect is when you can force any user to receive a malicious response which actually redirects the user to another domain.\r\n\r\nAs usual the Hackerone triager asked me for a PoC with a valid impact which is a fair enough request. From those 13 subdomains reported as potential vulnerable I was able to quickly found one vulnerable to Universal Redirect by just sending the request below.\r\nThe request above was pointed to one of the 13 subdomains. Since I cannot reveal anything regarding the company let‚Äôs say the requests was actually made to https://vulnerable.requestsmuggling.com. As you can see instead of using vulnerable.requestsmuggling.com on the Host headers I‚Äôve changed to www.example.com in order to get a redirect in the response pointed to it.\r\n\r\nBy playing the attacker with the request above the luckiest next user making any request to https://vulnerable.requestsmuggling.com would receive the response below generated by my malicious request.","ttr":"20 mins read","image":"https://thomastheblogger.s3.ap-south-1.amazonaws.com/48659271.jpeg","createdAt":{"$date":"2022-10-04T08:21:54.614Z"},"__v":0}
{"_id":{"$oid":"633bf99c6d9db40d51543ba1"},"title":"Everything you need to start with git","description":"As Git is everywhere , try to learn the basics which will help you","creator":{"$oid":"633bf92f6d9db40d51543b98"},"content":"Git, the word, originated from a British English slang that means a stupid or unpleasant person. Well, this ain‚Äôt the kind of Git we‚Äôre going to talk about today‚Ä¶\r\n\r\nIf you‚Äôre completely new to Git, this will help you get started with Git, understanding what Git is meant for and some fundamental concepts you need to understand about Git. I started with Git a few years ago, and in all honesty I had no idea what Git was for.\r\n\r\nI came across Git when I finally completed my NodeJS tutorial and decided to host an app on Heroku. I created a free account on Heroku and when I got to deploy, there were three options: Use Heroku cli, GitHub or Dropbox. Since I had come accross GitHub earlier, I chose it automatically. So I created an account and followed the instructions on how to upload a project into a repository. These instructions were merely followed without any knowledge to what I was doing, I just knew what I wanted to achieve and all I knew was ‚Äúfollow the instructions until you get to a point where it doesn‚Äôt work‚Äù.\r\n\r\nTo cut the long story short, it worked. And with that success, I challenged myself to understand this tool that had just gotten me to upload my first ever project to go live. I was ecstatic. It took me a long time to go through the Git docs, but in this article, it will take you less than 20 minutes to understand the basics of Git. This is not a full, hand‚Äôs on course. I might write a series about Git as a friend requested me to, but in this particular article, I‚Äôll just get you started on what Git is and why you need it.\r\nGit is a free and open source distributed version control system. Git‚Äôs purpose is to keep track of projects and files as they change over time with manipulations happening from different users. Git stores information about the project‚Äôs progress on a repository. A repository has commits to the project or a set of references to the commits called heads. All this information is stored in the same folder as the project in a sub-folder called .git and will mostly be hidden by default in most systems.\r\n\r\nSo basically. Git keeps track of the changes a couple of people make on a single project and then merges the code where people have worked on different parts into one project. This way, when someone introduces a bug, you can track down the code that introduced the bug by going through the commits. Commits must be made to a project to tell git that you‚Äôre satisfied with the changes you‚Äôve made and want to commit the changes into the main branch called master by default.\r\n\r\nYou can then upload the code to GitHub or BitBucket where authorised users can either view, pull the code or push changes.\r\n\r\nGetting started.\r\n\r\nTo get started with Git, you need to download it to your machine. Head over to https://git-scm.com/ and download the version most compatibe with your system.\r\n\r\nDuring the installation of Git, make sure you choose to run Git on the normal console window as well, this will enable you to run Git on your command prompt using the git command.\r\n\r\nOnce installed, open Git bash and create a working folder where you‚Äôll test out your project and keep track of changes. Once you ‚Äúcd‚Äù into the working folder, create a file and add random code to it. In my example I‚Äôll create a file called app.js and add the following lines of code.\r\nTo keep track of you changes, you can make changes to your code like add an extra statement or function. Assume that equivalent to adding around 600 lines of code to add a particular feature on a project like storing some data in localStorage or IndexedDB and manipulating it to fulfil a particular purpose. To keep track of when or who added this feature, you have to add changes to an index then when satisfied with the new lines, you commit the changes from the index to the main branch. Think of the index like a platform that keeps track of how far you‚Äôve gone before adding the incrementing lines of code collectively to the main project. To add files to an index after making changes use this command.","ttr":"10 mins read","image":"https://thomastheblogger.s3.ap-south-1.amazonaws.com/34561380.png","createdAt":{"$date":"2022-10-04T09:12:25.944Z"},"__v":0}
{"_id":{"$oid":"633bf0945337fd7e72331760"},"title":"Deep Dive into Amazon Simple Storage Service","description":"Amazon came up with an internet storage service called AWS S3. We will take you through this service in this AWS S3 tutorial blog","creator":{"$oid":"633bef835337fd7e72331756"},"content":"The AWS S3 tutorial will give you a clear understanding of the services along with some examples to which you can connect to.\r\n\r\nThe need for storage is increasing every day, so building and maintaining your own repositories, therefore, becomes a tedious and tiresome job because knowing the amount of capacity you may need in the future is difficult to predict. You may either over-utilize it leading to an application failure because of not having sufficient space or you may end up buying stacks of storage which will then be under-utilized.\r\n\r\nKeeping all these hassles in mind, Amazon came up with an internet storage service called AWS S3. We will take you through this service in this AWS S3 tutorial blog.\r\n\r\nAmazon Simple Storage Service (S3) is a storage for the internet. It is designed for large-capacity, low-cost storage provision across multiple geographical regions. Amazon S3 provides developers and IT teams with Secure, Durable and Highly Scalable object storage.\r\n\r\nS3 is Secure because AWS provides:\r\n\r\n    Encryption to the data that you store. It can happen in two ways:\r\n    Client Side Encryption\r\n    Server Side Encryption\r\n    Multiple copies are maintained to enable regeneration of data in case of data corruption\r\n    Versioning, wherein each edit is archived for a potential retrieval.\r\n\r\nS3 is Durable because:\r\n\r\n    It regularly verifies the integrity of data stored using checksums e.g. if S3 detects there is any corruption in data, it is immediately repaired with the help of replicated data.\r\n    Even while storing or retrieving data, it checks incoming network traffic for any corrupted data packets.\r\n\r\nS3 is Highly Scalable, since it automatically scales your storage according to your requirement and you only pay for the storage you use.\r\n\r\nThe next question which comes to our mind is,\r\n\r\nWhat kind and how much of data one can store in AWS S3?\r\n\r\nYou can store virtually any kind of data, in any format, in S3 and when we talk about capacity, the volume and the number of objects that we can store in S3 are unlimited.\r\n\r\n*An object is the fundamental entity in S3. It consists of data, key and metadata.\r\n\r\nWhen we talk about data, it can be of two types-\r\n\r\n    Data which is to be accessed frequently.\r\n    Data which is accessed not that frequently.\r\n\r\nTherefore, Amazon came up with 3 storage classes to provide its customers the best experience and at an affordable cost.\r\n\r\nHow is S3 billed?\r\n\r\nThough having so many features, AWS S3 is affordable and flexible in its costing. It works on Pay Per Use, meaning, you only pay what you use. The table below is an example for pricing of S3 for a specific region:\r\nIf you replicate 1,000 1 GB objects (1,000 GB) between regions you will incur a request charge of $0.005 (1,000 requests x $0.005 per 1,000 requests) for replicating 1,000 objects and a charge of $20 ($0.020 per GB transferred x 1,000 GB) for inter-region data transfer. After replication, the 1,000 GB will incur storage charges based on the destination region.\r\n\r\nSnowball, there are 2 variants:\r\n\r\n    Snowball 50 TB : 200$\r\n    Snowball 80 TB: 250$\r\n\r\nThis is the fixed service fee that they charge.\r\n\r\nApart from this there are on-site, charges which are exclusive of shipping days, the shipping days are free.\r\n\r\nThe first 10 on-site days are also free, meaning when the Snowball reaches your premises from then, till the day it is shipped back, they are the on-site days. The day it arrives, and the day it is shipped gets counted as shipping days, therefore are free.","ttr":"10 min read","image":"https://thomastheblogger.s3.ap-south-1.amazonaws.com/55983845.png","createdAt":{"$date":"2022-10-04T08:21:54.614Z"},"__v":0}
